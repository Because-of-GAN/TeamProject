{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage.filters import median_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언샵 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsharp(image, sigma, strength):\n",
    " \n",
    "    # Median filtering\n",
    "    image_mf = median_filter(image, sigma)\n",
    " \n",
    "    # Calculate the Laplacian\n",
    "    lap = cv2.Laplacian(image_mf,cv2.CV_64F)\n",
    " \n",
    "    # Calculate the sharpened image\n",
    "    sharp = image-strength*lap\n",
    "    \n",
    "    sharp[sharp>255] = 255\n",
    "    sharp[sharp<0] = 0\n",
    "    \n",
    "    return sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv function\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad)) # convolution 레이어입니다.\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))  # batch normalization 레이어를 추가해줍니다.\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# deconv function\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Unet encoder\n",
    "        self.conv1 = conv(3, 64, 4, bn=False)\n",
    "        self.conv2 = conv(64, 128, 4)\n",
    "        self.conv3 = conv(128, 256, 4)\n",
    "        self.conv4 = conv(256, 512, 4)\n",
    "        self.conv5 = conv(512, 512, 4)\n",
    "        self.conv6 = conv(512, 512, 4)\n",
    "        self.conv7 = conv(512, 512, 4)\n",
    "        self.conv8 = conv(512, 512, 4, bn=False)\n",
    "\n",
    "        # Unet decoder\n",
    "        self.deconv1 = deconv(512, 512, 4)\n",
    "        self.deconv2 = deconv(1024, 512, 4)\n",
    "        self.deconv3 = deconv(1024, 512, 4)\n",
    "        self.deconv4 = deconv(1024, 512, 4)\n",
    "        self.deconv5 = deconv(1024, 256, 4)\n",
    "        self.deconv6 = deconv(512, 128, 4)\n",
    "        self.deconv7 = deconv(256, 64, 4)\n",
    "        self.deconv8 = deconv(128, 3, 4)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # Unet encoder\n",
    "        e1 = self.conv1(input)\n",
    "        e2 = self.conv2(F.leaky_relu(e1, 0.2))\n",
    "        e3 = self.conv3(F.leaky_relu(e2, 0.2))\n",
    "        e4 = self.conv4(F.leaky_relu(e3, 0.2))\n",
    "        e5 = self.conv5(F.leaky_relu(e4, 0.2))\n",
    "        e6 = self.conv6(F.leaky_relu(e5, 0.2))\n",
    "        e7 = self.conv7(F.leaky_relu(e6, 0.2))\n",
    "        e8 = self.conv8(F.leaky_relu(e7, 0.2))\n",
    "                              \n",
    "        # Unet decoder\n",
    "        d1 = F.dropout(self.deconv1(F.relu(e8)), 0.5, training=True)\n",
    "        d2 = F.dropout(self.deconv2(F.relu(torch.cat([d1, e7], 1))), 0.5, training=True)\n",
    "        d3 = F.dropout(self.deconv3(F.relu(torch.cat([d2, e6], 1))), 0.5, training=True)\n",
    "        d4 = self.deconv4(F.relu(torch.cat([d3, e5], 1)))\n",
    "        d5 = self.deconv5(F.relu(torch.cat([d4, e4], 1)))\n",
    "        d6 = self.deconv6(F.relu(torch.cat([d5, e3], 1)))\n",
    "        d7 = self.deconv7(F.relu(torch.cat([d6, e2], 1)))\n",
    "        d8 = self.deconv8(F.relu(torch.cat([d7, e1], 1)))\n",
    "        output = torch.tanh(d8)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    \"\"\"Convert tensor to variable.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def to_data(x):\n",
    "    \"\"\"Convert variable to tensor.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def show_images(real_a,real_b,fake_b):\n",
    "    \n",
    "    plt.figure(figsize=(30,90))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(to_data(real_a).numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    #plt.imsave(f'sketch{num}',(to_data(real_a).numpy().transpose(1,2,0)))\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(to_data(real_b).numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    #plt.imsave(f'real_img{num}',(to_data(real_a).numpy().transpose(1,2,0)))\n",
    "    \n",
    "    #plt.imsave(f'fake_img{num}',(to_data(real_a).numpy().transpose(1,2,0)))\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(to_data(fake_b).numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    #plt.savefig(f'img{num}.jpg',bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "def save_images(fake_b,num):\n",
    "    plt.figure(figsize=(30,90))\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(to_data(fake_b).numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(f'{num}.jpg',bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "def upgradepic(a):\n",
    "    blur = cv2.GaussianBlur(a,(3,3),0)\n",
    "    median = cv2.medianBlur(blur,5)\n",
    "    blur2 = cv2.bilateralFilter(median,-1,15,15)\n",
    "    return blur2;    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "G = Generator()\n",
    "G.load_state_dict(torch.load(\"./hed_G_500\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/facades/skt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_skt = \"D:/facades/skt5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_img= \"D:/facades/img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirlist_img= os.listdir('D:/facades/img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirlist_skt = os.listdir('D:/facades/skt5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for filename in dirlist_skt:\n",
    "    facade_a = Image.open(os.path.join(directory_skt,filename)).convert('RGB')\n",
    "    facade_a = facade_a.resize((256, 256), Image.BICUBIC)\n",
    "    facade_a = transforms.ToTensor()(facade_a) # Quiz\n",
    "    facade_a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(facade_a)\n",
    "    fake_facade = G(facade_a.expand(1,3,256,256))\n",
    "    \n",
    "    facade_b = Image.open(os.path.join(directory_img,filename)).convert('RGB')\n",
    "    facade_b = facade_b.resize((256, 256), Image.BICUBIC)\n",
    "    facade_b = transforms.ToTensor()(facade_b) # Quiz\n",
    "    facade_b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(facade_b)\n",
    "    \n",
    "    \n",
    "    show_images(denorm(facade_a.squeeze()), denorm(facade_b.squeeze()),denorm(fake_facade.squeeze()),)\n",
    "    \n",
    "    save_images(denorm(fake_facade.squeeze()),filename)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unsharp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirlist = os.listdir('C:/Users/User/Desktop/데이터진흥원/d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for e,i in enumerate(dirlist):\n",
    "    img2 = os.path.join('C:/Users/User/Desktop/데이터진흥원/d',i)\n",
    "    original_image = plt.imread(img2)\n",
    "    sharp1 = np.zeros_like(original_image)\n",
    "    for i in range(3):\n",
    "        sharp1[:,:,i] = unsharp(original_image[:,:,i], 5, 0.8)\n",
    "    \n",
    "    plt.imsave(f'sharp{e}.jpg',sharp1)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
