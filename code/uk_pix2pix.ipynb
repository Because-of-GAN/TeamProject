{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "uk_pix2pix.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUVq4nQPRJam",
        "colab_type": "text"
      },
      "source": [
        "# Pix2Pix 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG6kG977_S2a",
        "colab_type": "code",
        "outputId": "29258c00-5701-43ce-ede4-994255ddfa95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCmx3juk_oLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjCbonn7_nsq",
        "colab_type": "text"
      },
      "source": [
        "구글 드라이브 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urGD-v1URJan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from math import log10\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # tensor.to(device) 방식을 통해서 cpu -> gpu로 보낼 수 있습니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flQLI9TZRJaq",
        "colab_type": "text"
      },
      "source": [
        "## Facades Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlsTpYtHRJaq",
        "colab_type": "text"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDAVW6KORJar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image를 Load하기 위한 함수입니다.\n",
        "def load_img(filepath):\n",
        "    img = Image.open(filepath).convert('RGB') # Image File을 Load합니다.\n",
        "    img = img.resize((256, 256), Image.BICUBIC) # Image의 크기를 256x256으로 바꿔줍니다.\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "MNIST 또는 CIFAR-10과 같이 torchvision 라이브러리에서 제공하지 않는 Dataset의 경우에는 직접 data.Dataset을 상속받아서 만들어야합니다.\n",
        "이 때, __init__, __getitem__, __len__은 항상 새로 정의해줘야 합니다.\n",
        "\"\"\"\n",
        "class DatasetFromFolder(data.Dataset):\n",
        "    def __init__(self, image_dir, direction):\n",
        "        super(DatasetFromFolder, self).__init__()\n",
        "        self.direction = direction\n",
        "        self.a_path = os.path.join(image_dir, \"skt\") # a는 건물 사진\n",
        "        self.b_path = os.path.join(image_dir, \"img\") # b는 Segmentation Mask\n",
        "        self.a_filenames = sorted([x for x in os.listdir(self.a_path)]) # a 폴더에 있는 파일 목록\n",
        "        self.b_filenames = sorted([x for x in os.listdir(self.b_path)]) # a 폴더에 있는 파일 목록\n",
        "\n",
        "\n",
        "    # getitem 함수는 index에 맞는 data를 반환하는 역할을 합니다.\n",
        "    def __getitem__(self, index):\n",
        "        # a와 b 폴더에서 각각 건물 사진과 Segmentation Mask를 Load합니다.\n",
        "        a = Image.open(os.path.join(self.a_path, self.a_filenames[index])).convert('RGB')\n",
        "        b = Image.open(os.path.join(self.b_path, self.b_filenames[index])).convert('RGB')\n",
        "        \n",
        "        # a(건물사진)와 b(Segmentation Mask) 이미지를 각각 Resize를 하고, Tensor로 바꿔줍니다.\n",
        "        a = a.resize((286, 286), Image.BICUBIC)\n",
        "        b = b.resize((286, 286), Image.BICUBIC)\n",
        "        a = transforms.ToTensor()(a) # Quiz\n",
        "        b = transforms.ToTensor()(b) # Quiz\n",
        "        \n",
        "        # Data Augmentation\n",
        "        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
        "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
        "    \n",
        "        # a와 b 이미지를 256x256 크기로 Crop을 합니다.(잘라내기)\n",
        "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
        "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
        "    \n",
        "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
        "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
        "            idx = torch.LongTensor(idx)\n",
        "            a = a.index_select(2, idx)\n",
        "            b = b.index_select(2, idx)\n",
        "\n",
        "        if self.direction == \"a2b\":\n",
        "            return a, b\n",
        "        else:\n",
        "            return b, a\n",
        "\n",
        "    # len 함수는 Dataset 전체 개수를 반환합니다.\n",
        "    def __len__(self):\n",
        "        return len(self.a_filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0b3gmaHRJat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_set(root_dir, direction):\n",
        "    train_dir = os.path.join(root_dir, \"train\")\n",
        "\n",
        "    return DatasetFromFolder(train_dir, direction)\n",
        "\n",
        "\n",
        "def get_test_set(root_dir, direction):\n",
        "    test_dir = os.path.join(root_dir, \"test\")\n",
        "\n",
        "    return DatasetFromFolder(test_dir, direction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqJC83VNKv2c",
        "colab_type": "code",
        "outputId": "c81f8f6f-31dc-4a5e-aba3-e70accab79b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "os.listdir('./brick')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1KIkrPPRJav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = get_training_set(\"./brick\", \"a2b\")\n",
        "#test_set = get_test_set(\"./brick\", \"a2b\")\n",
        "\n",
        "# Training Set과 Test Set을 각각 DataLoader에 넣습니다.\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=1, shuffle=True)\n",
        "#testing_data_loader = DataLoader(dataset=test_set, num_workers=1, batch_size=1, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ8j7-wYRJax",
        "colab_type": "text"
      },
      "source": [
        "## Pix2Pix Generator, Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oetOc9FuRJay",
        "colab_type": "text"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34IaYJJERJay",
        "colab_type": "text"
      },
      "source": [
        "## U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeHUFXazRJaz",
        "colab_type": "text"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb-SVm6wRJaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conv function\n",
        "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
        "    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n",
        "    layers = []\n",
        "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad)) # convolution 레이어입니다.\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))  # batch normalization 레이어를 추가해줍니다.\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# deconv function\n",
        "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
        "    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n",
        "    layers = []\n",
        "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xARyU4VRJa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Unet encoder\n",
        "        self.conv1 = conv(3, 64, 4, bn=False)\n",
        "        self.conv2 = conv(64, 128, 4)\n",
        "        self.conv3 = conv(128, 256, 4)\n",
        "        self.conv4 = conv(256, 512, 4)\n",
        "        self.conv5 = conv(512, 512, 4)\n",
        "        self.conv6 = conv(512, 512, 4)\n",
        "        self.conv7 = conv(512, 512, 4)\n",
        "        self.conv8 = conv(512, 512, 4, bn=False)\n",
        "\n",
        "        # Unet decoder\n",
        "        self.deconv1 = deconv(512, 512, 4)\n",
        "        self.deconv2 = deconv(1024, 512, 4)\n",
        "        self.deconv3 = deconv(1024, 512, 4)\n",
        "        self.deconv4 = deconv(1024, 512, 4)\n",
        "        self.deconv5 = deconv(1024, 256, 4)\n",
        "        self.deconv6 = deconv(512, 128, 4)\n",
        "        self.deconv7 = deconv(256, 64, 4)\n",
        "        self.deconv8 = deconv(128, 3, 4)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        # Unet encoder\n",
        "        e1 = self.conv1(input)\n",
        "        e2 = self.conv2(F.leaky_relu(e1, 0.2))\n",
        "        e3 = self.conv3(F.leaky_relu(e2, 0.2))\n",
        "        e4 = self.conv4(F.leaky_relu(e3, 0.2))\n",
        "        e5 = self.conv5(F.leaky_relu(e4, 0.2))\n",
        "        e6 = self.conv6(F.leaky_relu(e5, 0.2))\n",
        "        e7 = self.conv7(F.leaky_relu(e6, 0.2))\n",
        "        e8 = self.conv8(F.leaky_relu(e7, 0.2))\n",
        "                              \n",
        "        # Unet decoder\n",
        "        d1 = F.dropout(self.deconv1(F.relu(e8)), 0.5, training=True)\n",
        "        d2 = F.dropout(self.deconv2(F.relu(torch.cat([d1, e7], 1))), 0.5, training=True)\n",
        "        d3 = F.dropout(self.deconv3(F.relu(torch.cat([d2, e6], 1))), 0.5, training=True)\n",
        "        d4 = self.deconv4(F.relu(torch.cat([d3, e5], 1)))\n",
        "        d5 = self.deconv5(F.relu(torch.cat([d4, e4], 1)))\n",
        "        d6 = self.deconv6(F.relu(torch.cat([d5, e3], 1)))\n",
        "        d7 = self.deconv7(F.relu(torch.cat([d6, e2], 1)))\n",
        "        d8 = self.deconv8(F.relu(torch.cat([d7, e1], 1)))\n",
        "        output = torch.tanh(d8)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-JzVLhLRJa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = conv(6, 64, 4, bn=False)\n",
        "        self.conv2 = conv(64, 128, 4)\n",
        "        self.conv3 = conv(128, 256, 4)\n",
        "        self.conv4 = conv(256, 512, 4, 1, 1)\n",
        "        self.conv5 = conv(512, 1, 4, 1, 1)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        out = F.leaky_relu(self.conv1(input), 0.2)\n",
        "        out = F.leaky_relu(self.conv2(out), 0.2)\n",
        "        out = F.leaky_relu(self.conv3(out), 0.2)\n",
        "        out = F.leaky_relu(self.conv4(out), 0.2)\n",
        "        out = torch.sigmoid(self.conv5(out))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwOSqB2DRJa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeT-bgJHRJa7",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqApUN7RJa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterionL1 = nn.L1Loss().to(device)\n",
        "criterionMSE = nn.MSELoss().to(device)\n",
        "\n",
        "# setup optimizer\n",
        "optimizer_g = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j07rpGE3RJa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_var(x):\n",
        "    \"\"\"Convert tensor to variable.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x)\n",
        "\n",
        "def to_data(x):\n",
        "    \"\"\"Convert variable to tensor.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data\n",
        "\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def show_images(real_a, real_b, fake_b):\n",
        "    plt.figure(figsize=(30,90))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(to_data(real_a).numpy().transpose(1,2,0))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    plt.imshow(to_data(real_b).numpy().transpose(1,2,0))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    plt.imshow(to_data(fake_b).numpy().transpose(1,2,0))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQpndma_9GFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RYfHLGjXRJbA",
        "colab_type": "code",
        "outputId": "c94731a0-730d-4d18-95dd-84e79a6fa9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "for epoch in range(1, 501):\n",
        "    # train\n",
        "    for iteration, batch in enumerate(training_data_loader, 1):\n",
        "        # forward\n",
        "        real_a, real_b = batch[0].to(device), batch[1].to(device)\n",
        "        fake_b = G(real_a)\n",
        "        \n",
        "        real_label = to_var(torch.ones(1))\n",
        "        fake_label = to_var(torch.zeros(1))\n",
        "\n",
        "        #============= Train the discriminator =============#\n",
        "\n",
        "        optimizer_d.zero_grad()\n",
        "        \n",
        "        # train with fake\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = D.forward(fake_ab.detach())\n",
        "        loss_d_fake = criterionMSE(pred_fake, fake_label)\n",
        "\n",
        "        # train with real\n",
        "        real_ab = torch.cat((real_a, real_b), 1)\n",
        "        pred_real = D.forward(real_ab)\n",
        "        loss_d_real = criterionMSE(pred_real, real_label)\n",
        "        \n",
        "        # Combined D loss\n",
        "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
        "\n",
        "        loss_d.backward()\n",
        "       \n",
        "        optimizer_d.step()\n",
        "\n",
        "        #=============== Train the generator ===============#\n",
        "\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = D.forward(fake_ab)\n",
        "        loss_g_gan = criterionMSE(pred_fake, real_label)\n",
        "\n",
        "        # Second, G(A) = B\n",
        "        loss_g_l1 = criterionL1(fake_b, real_b) * 10\n",
        "        \n",
        "        loss_g = loss_g_gan + loss_g_l1\n",
        "        \n",
        "        loss_g.backward()\n",
        "\n",
        "        optimizer_g.step()\n",
        "\n",
        "        #show_images(denorm(real_a.squeeze()), denorm(real_b.squeeze()), denorm(fake_b.squeeze()))\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n",
        "            epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\n",
        "           \n",
        "            \n",
        "        if epoch % 50 == 0:\n",
        "            torch.save(D.state_dict(), f'./save_model2/Canny_D_{epoch}')\n",
        "            torch.save(G.state_dict(), f'./save_model2/Canny_G_{epoch}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 30, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFi_oecuyxqn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX8KzZWSYK2e",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb0ZXVYQZGxU",
        "colab_type": "code",
        "outputId": "c054574c-d10d-47cd-b98a-9fbfc269d683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "G =Generator()\n",
        "D = Discriminator()\n",
        "D.load_state_dict(torch.load(\"./save_model2/con_D_500\", map_location=\"cuda:0\")) \n",
        "G.load_state_dict(torch.load(\"./save_model2/con_G_500\", map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
        "D.to(device)\n",
        "G.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv5): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv6): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv7): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv8): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (deconv1): Sequential(\n",
              "    (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv2): Sequential(\n",
              "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv3): Sequential(\n",
              "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv4): Sequential(\n",
              "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv5): Sequential(\n",
              "    (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv6): Sequential(\n",
              "    (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv7): Sequential(\n",
              "    (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (deconv8): Sequential(\n",
              "    (0): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKPotsPnRJbD",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDpHsfL8RJbE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}